{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QshK8s21WBrf"
      },
      "source": [
        "# Week 08\n",
        "\n",
        "Regression modeling and Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Hf8SXUwWOho"
      },
      "source": [
        "## Setup\n",
        "\n",
        "Run the following 2 cells to import all necessary libraries and helpers for this week's exercises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -q https://github.com/DM-GY-9103-2024F-H/9103-utils/raw/main/src/data_utils.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "from data_utils import StandardScaler\n",
        "from data_utils import PolynomialFeatures\n",
        "from data_utils import LinearRegression, RandomForestClassifier\n",
        "from data_utils import regression_error\n",
        "from data_utils import object_from_json_url"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prKGt8bzScNA"
      },
      "source": [
        "## Regression\n",
        "\n",
        "Regression, or Regression Analysis, is a set of statistical processes for estimating the relationship between a dependent variable (sometimes called the 'outcome', 'response' or 'label') and one or more independent variables (called 'features', 'dimensions' or 'columns').\n",
        "\n",
        "For example, let's say we have the following data about people's wages and years of experience:\n",
        "\n",
        "<img src=\"./imgs/wages-exp.png\" width=\"620px\"/>\n",
        "\n",
        "We could use regression to calculate how the values for wages are affected by years of experience in our dataset, and then create a function to more generally estimate the relation between wages and experience:\n",
        "\n",
        "<img src=\"./imgs/wages-exp-fit.png\" width=\"620px\"/>\n",
        "\n",
        "We could now estimate wages for values of years of experience that we didn't have measurements for.\n",
        "\n",
        "This is an estimate, but the more points we use and the more features we have in our dataset the better the regression results will be."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setting up Regression\n",
        "\n",
        "For a simple dataset we can perform regression by following these steps:\n",
        "\n",
        "1. Load dataset\n",
        "2. Encode label features as numbers\n",
        "3. Normalize the data\n",
        "4. Separate the outcome variable and the feature variables\n",
        "5. Create a regression model\n",
        "6. Run model on input data and \n",
        "7. Measure error"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Diamond Prices\n",
        "\n",
        "Let's use the dataset from last week to set up a diamond price estimator.\n",
        "\n",
        "Steps 1 - 3 should look familiar, and they've been pasted below:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 1. Load Dataset\n",
        "DIAMONDS_FILE = \"https://raw.githubusercontent.com/DM-GY-9103-2024F-H/9103-utils/main/datasets/json/diamonds.json\"\n",
        "\n",
        "# Read into DataFrame\n",
        "diamonds_data = object_from_json_url(DIAMONDS_FILE)\n",
        "diamonds_df = pd.DataFrame.from_records(diamonds_data)\n",
        "\n",
        "\n",
        "## 2. Encode non-numeric values\n",
        "cut_order = ['Fair', 'Good', 'Very Good', 'Premium', 'Ideal']\n",
        "color_order = ['J', 'I', 'H', 'G', 'F', 'E', 'D']\n",
        "clarity_order = ['I1', 'SI2', 'SI1', 'VS2', 'VS1', 'VVS2', 'VVS1', 'IF']\n",
        "\n",
        "diamond_encoder = OrdinalEncoder(categories=[cut_order, color_order, clarity_order])\n",
        "\n",
        "ccc_vals = diamond_encoder.fit_transform(diamonds_df[[\"cut\", \"color\", \"clarity\"]].values)\n",
        "diamonds_df[[\"cut\", \"color\", \"clarity\"]] = ccc_vals\n",
        "\n",
        "\n",
        "## 3. Normalize\n",
        "diamond_scaler = StandardScaler()\n",
        "diamonds_scaled = diamond_scaler.fit_transform(diamonds_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Chose Features\n",
        "\n",
        "Now we separate the outcome variable values and the independent variables.\n",
        "\n",
        "Let's start simple and use only one feature: `carat`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. Separate the outcome variable and the independent variables\n",
        "prices = diamonds_scaled[\"price\"]\n",
        "carats = diamonds_scaled[[\"carat\"]]\n",
        "\n",
        "# Plot the variables, just for checking\n",
        "plt.scatter(carats, prices, marker='o', linestyle='', alpha=0.3)\n",
        "plt.xlabel(\"carat\")\n",
        "plt.ylabel(\"price\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model\n",
        "\n",
        "Let's setup and create a linear regression model.\n",
        "\n",
        "We'll use the [LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) object from the `Scikit-Learn` library.\n",
        "\n",
        "Once we create an instance of a `LinearRegression` object, we can use its `fit()` function to compute the relationship between `price` and `carat` values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 5. Create a LinearRegression object\n",
        "price_model = LinearRegression()\n",
        "\n",
        "# Create a model that relates price of diamonds to their carat value\n",
        "price_model.fit(carats, prices)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluate\n",
        "\n",
        "It's really rare for a model to perfectly capture all of the subtleties of a dataset. Models try to capture trends in the data and not all data in a dataset is gonna fit a trend.\n",
        "\n",
        "When we run `predict()` on a dataset some/most of the predictions are gonna be off. Since we have the real values and the predicted values we can calculate how far off the predictions are.\n",
        "\n",
        "The simplest and most obvious way to do this is to feed the features from our dataset back into our model, but this time, instead of using them to train our model, we'll compare the computed prediction values to the actual known values of the outcome variables in our dataset.\n",
        "\n",
        "One way to formally quantify this process is to calculate the *Mean Squared Error* of our predictions, or, the average of the squares of individual errors.\n",
        "\n",
        "It's calculated using the known values for the outcome variable $Y$, and the predicted values from our model $\\hat{Y}$. For each prediction, we subtract the predicted value from the known value and square this difference. We do this for every predicted value, and sum all of these squared errors before dividing by the total number of predictions.\n",
        "\n",
        "It can be expressed as:\n",
        "\n",
        "$\\displaystyle \\frac{1}{n} \\sum_{i=1}^{n}\\left(Y_i - \\hat{Y}_i \\right)^{2}$\n",
        "\n",
        "Luckily, the `regression_error()` function does this for us when given the $2$ lists of outcome values, $Y$ and $\\hat{Y}$.\n",
        "\n",
        "We just have to be a bit careful abut which $2$ lists we use because the results from our model are normalized, and we have normalized and un-normalized versions of our original dataset. We can compare the normalized predictions to the normalized values from the dataset, but the resulting numbers will be less intuitive and a bit harder to interpret right away.\n",
        "\n",
        "### Un-Normalize\n",
        "\n",
        "It might be better to perform an inverse normalization transformation on the predicted values and compare those to the original values from our dataset. If we used a `MinMaxScaler` we should now transform the predictions values, between $0$ and $1$, back to their original units. If we used `StandardScaler` we should convert the predicted values, expressed in standard deviations, back to the original units of the outcome varible.\n",
        "\n",
        "Like all other `scikit-learn` scalers, our `diamond_scaler` object has an `inverse_transform()` function that will undo the scaling on any list of numbers we give to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 6. Run the model on the training data\n",
        "predicted_scaled = price_model.predict(carats)\n",
        "\n",
        "# Un-normalize the data\n",
        "predicted = diamond_scaler.inverse_transform(predicted_scaled)\n",
        "\n",
        "## 7. Measure error\n",
        "regression_error(diamonds_df[\"price\"], predicted[\"price\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Result\n",
        "\n",
        "Hmmm.... what this means is that on average our model is wrong by $\\$1388$ dollars.\n",
        "\n",
        "We can plot our predictions with the original data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the original values\n",
        "plt.scatter(carats, prices, marker='o', linestyle='', alpha=0.3)\n",
        "plt.xlabel(\"carat\")\n",
        "plt.ylabel(\"price\")\n",
        "\n",
        "# Plot the predictions\n",
        "plt.scatter(carats, predicted_scaled, color='r', marker='o', linestyle='', alpha=0.05)\n",
        "plt.xlabel(\"carat\")\n",
        "plt.ylabel(\"price\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "We're only using one variable to model the price using linear regression, so, as the name suggests, the resulting model is a line:\n",
        "\n",
        "$\\displaystyle price = \\beta \\cdot carat$\n",
        "\n",
        "(This should look familiar; it's the equation for a line that we might have learned in algebra: $y = m \\cdot x + b$)\n",
        "\n",
        "$\\beta$ in this equation is a constant calculated by the model. The model uses all of the values about `price` and `carat` to calculate this ONE constant that defines our line.\n",
        "\n",
        "For every value of $carat$ the model gives us a value for $price$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Another Graph\n",
        "\n",
        "Another way to visualize how far off the model is from the actual data is to look at sorted lists of prices.\n",
        "\n",
        "So, we run the model on the input, get the resulting prices, sort them and plot them against the original sorted prices.\n",
        "\n",
        "This is just a quick way to see if there are regions of our data where the model is wrong more often, or right more often. We can combine that information with the distribution of prices in the dataset to see if it's ok for our model to be this wrong.\n",
        "\n",
        "We could have a situation where very rare and uncommon diamond prices (too low or too high) are contributing a large amount of error to our average error. If the predictions for the more common diamond prices are close, then maybe our model is good."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prices_original = diamonds_df[\"price\"]\n",
        "prices_predicted = predicted[\"price\"]\n",
        "\n",
        "# Plot the original and predicted prices\n",
        "plt.plot(sorted(prices_original), marker='o', linestyle='', alpha=0.3)\n",
        "plt.plot(sorted(prices_predicted), color='r', marker='o', markersize='3', linestyle='', alpha=0.1)\n",
        "plt.ylabel(\"price\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "The model seems ok with lower priced diamonds, but anything more expensive than $\\$6000$ seems to be more wrong.\n",
        "\n",
        "Let's add features to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using more features\n",
        "\n",
        "Let's use a few more features to build our model.\n",
        "\n",
        "This time our model equation will have multiple independent variables:\n",
        "\n",
        "$\\displaystyle price = \\beta_0 \\cdot carat + \\beta_1 \\cdot width + \\beta_2 \\cdot length$\n",
        "\n",
        "The $\\beta_i$ values in this equation are constants that the model calculates. There are $3$ now, so the model has more parameters to adjust in order to get a better fit.\n",
        "\n",
        "The overall process for preparing our data is the same, but this time we'll use `carat` along with the `x` and `y` dimensions of the diamond to predict prices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. Separate the outcome variable and the independent variables\n",
        "prices = diamonds_scaled[\"price\"]\n",
        "features = diamonds_scaled[[\"carat\", \"x\", \"y\"]]\n",
        "\n",
        "## 5. Create a LinearRegression object\n",
        "price_model = LinearRegression()\n",
        "\n",
        "# Create a model that relates price of diamonds to their carat value as well as width and length\n",
        "price_model.fit(features, prices)\n",
        "\n",
        "## 6. Run the model on the training data\n",
        "predicted_scaled = price_model.predict(features)\n",
        "\n",
        "# Un-normalize the data\n",
        "predicted = diamond_scaler.inverse_transform(predicted_scaled)\n",
        "\n",
        "## 7. Measure error\n",
        "regression_error(diamonds_df[\"price\"], predicted[\"price\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Result\n",
        "\n",
        "This is better. The error decreased.\n",
        "\n",
        "We can plot our new model, but since we humans are limited to $3$ physical dimensions that we can comprehend, we can't plot price as a function of all $3$ of our features.\n",
        "\n",
        "We'll create a $3D$ plot that to look at how price varies along with the length and width of the diamonds:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(8, 8))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "\n",
        "xs = diamonds_df[[\"x\"]].values\n",
        "ys = diamonds_df[[\"y\"]].values\n",
        "\n",
        "ps = diamonds_df[[\"price\"]].values\n",
        "pps = predicted[[\"price\"]].values\n",
        "\n",
        "ax.scatter(xs, ys, ps, marker='o', linestyle='', alpha=0.1)\n",
        "ax.scatter(xs, ys, pps, color='r', marker='o', linestyle='', alpha=0.1)\n",
        "\n",
        "ax.set_xlabel('width')\n",
        "ax.set_ylabel('length')\n",
        "ax.set_zlabel('price')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using ALL features\n",
        "\n",
        "Let's use all $9$ features from the dataset to build our model.\n",
        "\n",
        "The model equation will be something like:\n",
        "\n",
        "$\\displaystyle price = \\beta_0 \\cdot x_0 + \\beta_1 \\cdot x_1 + ... + \\beta_8 \\cdot x_8$\n",
        "\n",
        "Where the $x_i$ values are the values of our features (`carat`, `width`, etc) and the $\\beta_i$ parameters are the constant values that the model will calculate.\n",
        "\n",
        "Repeat the steps below, but this time using all of the features in our dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: repeat steps for training a predictor on all features\n",
        "\n",
        "## 4. Separate the outcome variable and the independent variables\n",
        "# Use all features *except* price\n",
        "prices = diamonds_scaled[\"price\"]\n",
        "features = diamonds_scaled.drop(columns=[\"price\"])\n",
        "\n",
        "## 5. Create a LinearRegression object\n",
        "price_model = LinearRegression()\n",
        "\n",
        "# Create a model that relates price of diamonds to many features\n",
        "price_model.fit(features, prices)\n",
        "\n",
        "## 6. Run the model on the training data\n",
        "predicted_scaled = price_model.predict(features)\n",
        "\n",
        "# Un-normalize the data\n",
        "predicted = diamond_scaler.inverse_transform(predicted_scaled)\n",
        "\n",
        "## 7. Measure error\n",
        "regression_error(diamonds_df[\"price\"], predicted[\"price\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Result\n",
        "\n",
        "The error should have gotten better.\n",
        "\n",
        "Let's sort and plot all of the prices from the original dataset and the reconstructed prices from our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prices_original = diamonds_df[\"price\"]\n",
        "prices_predicted = predicted[\"price\"]\n",
        "\n",
        "# Plot the original and predicted prices\n",
        "plt.plot(sorted(prices_original), marker='o', linestyle='', alpha=0.3)\n",
        "plt.plot(sorted(prices_predicted), color='r', marker='o', markersize='3', linestyle='', alpha=0.1)\n",
        "plt.ylabel(\"price\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "This is another one of these graphs of sorted prices that can be used to see the distribution of prices from the dataset and compare it to the prices from the model.\n",
        "\n",
        "Depending on where on the distribution our error comes from, we can have an ok model with large errors, as long as the errors are in regions that aren't common.\n",
        "\n",
        "In this case, there is some pretty constant error throughout the modeled prices.\n",
        "\n",
        "The error is even worse for diamonds on the extreme ends of price: the too cheap and too expensive ones.\n",
        "\n",
        "And since even a small percentage of error for an expensive diamond contributes to a large error in dollars, this is probably where a lot of the error is coming from."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Even MORE Features !\n",
        "\n",
        "One trick to improve our model is to create some extra features from the current ones.\n",
        "\n",
        "For example, in addition to considering `carat` and `width` of each diamond separately, we can create a feature that is a combination of these two values.\n",
        "\n",
        "Considering just those $2$ features, instead of having an equation that is like this:\n",
        "\n",
        "$\\displaystyle price = \\beta_0 \\cdot carat + \\beta_1 \\cdot width$\n",
        "\n",
        "We can try to model an equation that has quadratic terms, like this:\n",
        "\n",
        "$\\displaystyle price = \\beta_0 \\cdot carat + \\beta_1 \\cdot width + \\beta_2 \\cdot carat^2 + \\beta_3 \\cdot width^2 + \\beta_4 \\cdot carat \\cdot width$\n",
        "\n",
        "Or even cubic terms:\n",
        "\n",
        "$\\displaystyle price = \\beta_0 \\cdot carat + \\beta_1 \\cdot width + \\beta_2 \\cdot carat^2 + \\beta_3 \\cdot width^2 + \\beta_4 \\cdot carat \\cdot width + \\beta_5 \\cdot carat^3 + \\beta_6 \\cdot width^3$\n",
        "\n",
        "This allows our model to figure out more complex relationships between the features and consider non-linear relationships between features and price (maybe `price` goes up proportional to the square of the `width` of the diamond).\n",
        "\n",
        "Scikit-Learn has an object called [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) that helps us do exactly this. We just have to instantiate it and use it to create some extra features for us.\n",
        "\n",
        "The process of using it should loo familiar by now: instantiate an object, use its `fit_transform()` function on our features to get a new version of our data, use the new data, success."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. Separate the outcome variable and the independent variables\n",
        "prices = diamonds_scaled[\"price\"]\n",
        "features = diamonds_scaled.drop(columns=[\"price\"])\n",
        "\n",
        "## 4B. Create extra features\n",
        "poly = PolynomialFeatures(degree=3, include_bias=False)\n",
        "features_poly = poly.fit_transform(features)\n",
        "\n",
        "## 5. Create a LinearRegression object\n",
        "price_model = LinearRegression()\n",
        "\n",
        "# Create a model that relates price of diamonds to many features\n",
        "price_model.fit(features_poly, prices)\n",
        "\n",
        "## 6. Run the model on the training data\n",
        "predicted_scaled = price_model.predict(features_poly)\n",
        "\n",
        "# Un-normalize the data\n",
        "predicted = diamond_scaler.inverse_transform(predicted_scaled)\n",
        "\n",
        "## 7. Measure error\n",
        "regression_error(diamonds_df[\"price\"], predicted[\"price\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Result\n",
        "\n",
        "This is significantly better than our original model.\n",
        "\n",
        "Let's sort and plot the resulting prices again:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot sorted prices\n",
        "prices_original = sorted(diamonds_df[\"price\"])\n",
        "prices_predicted = sorted(predicted[\"price\"])\n",
        "\n",
        "# Plot the original and predicted prices\n",
        "plt.plot(prices_original, marker='o', linestyle='', alpha=0.3)\n",
        "plt.plot(prices_predicted, color='r', marker='o', markersize='3', linestyle='', alpha=0.1)\n",
        "plt.ylabel(\"price\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And this looks good, except for very cheap and very expensive diamonds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### More plots\n",
        "\n",
        "And since we can't see in 4D or 5D yet, let's just plot `price` as a function of a few individual features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot price vs carat, x, y and z\n",
        "for feat in [\"carat\", \"x\", \"y\", \"z\"]:\n",
        "  x = diamonds_df[feat]\n",
        "  prices_original = diamonds_df[\"price\"]\n",
        "  prices_predicted = predicted[\"price\"]\n",
        "\n",
        "  # Plot the original and predicted prices\n",
        "  plt.plot(x, prices_original, marker='o', linestyle='', alpha=0.3)\n",
        "  plt.plot(x, prices_predicted, color='r', marker='o', markersize='3', linestyle='', alpha=0.1)\n",
        "  plt.xlabel(feat)\n",
        "  plt.ylabel(\"price\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🎉🍾\n",
        "\n",
        "These look ok. We have a nice model for diamond prices."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## More Non-Linear Linear Regression!\n",
        "\n",
        "This is just a quick aside into the world of _**feature engineering**_, which is the process of carefully adding data to our dataset, based on data we already have.\n",
        "\n",
        "Let's repeat the process of linear regression for a new dataset for monthly diamond sales.\n",
        "\n",
        "We'll follow the same steps as before:\n",
        "1. Load data\n",
        "2. Encode\n",
        "3. Normalize\n",
        "4. Choose features\n",
        "5. Create model\n",
        "6. Test model on input data\n",
        "7. Measure error and analyze results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 1. Load Dataset\n",
        "SALES_FILE = \"https://raw.githubusercontent.com/DM-GY-9103-2024F-H/9103-utils/main/datasets/json/diamond_sales.json\"\n",
        "\n",
        "# Read into DataFrame\n",
        "sales_data = object_from_json_url(SALES_FILE)\n",
        "sales_df = pd.DataFrame.from_records(sales_data)\n",
        "\n",
        "# Look at features: values, types, names, min, max, mean\n",
        "for c in sales_df.columns:\n",
        "  print(c, \"\\n\\tmin:\", sales_df[c].min())\n",
        "  print(\"\\tmax:\", sales_df[c].max())\n",
        "  print(\"\\tavg:\", round(sales_df[c].mean(), 3))\n",
        "  print(\"\\tstd:\", round(sales_df[c].std(), 3))\n",
        "\n",
        "sales_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is a much simpler dataset with only one feature: `month`, and one outcome: `sales`.\n",
        "\n",
        "We can easily plot all of the data just to check for any trends or specificities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(sales_df[\"month\"], sales_df[\"sales\"], marker='o', linestyle='', alpha=0.3)\n",
        "plt.title(\"monthly sales\")\n",
        "plt.xlabel(\"month\")\n",
        "plt.ylabel(\"sales\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looks like diamond sales to me !\n",
        "\n",
        "Despite the spread of sale amounts between $\\$1000$ and $\\$6000$, we can kind of notice a slightly upward trend.\n",
        "\n",
        "Let's check with a model.\n",
        "\n",
        "### Model\n",
        "\n",
        "Let's do linear regression to predict future diamond sales amounts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: train a Linear Regression predictor for sales based on month\n",
        "\n",
        "## 3. Normalize\n",
        "sales_scaler = StandardScaler()\n",
        "sales_scaled = sales_scaler.fit_transform(sales_df)\n",
        "\n",
        "## 4. Separate the outcome variable and the independent variables\n",
        "sales = sales_scaled[\"sales\"]\n",
        "months = sales_scaled[[\"month\"]]\n",
        "\n",
        "## 5. Create a LinearRegression object\n",
        "sales_model = LinearRegression()\n",
        "\n",
        "# Use the object to create a model that relates sales amounts to month\n",
        "sales_model.fit(months, sales)\n",
        "\n",
        "## 6. Run the model on the training data\n",
        "predicted_scaled = sales_model.predict(months)\n",
        "\n",
        "# Un-normalize the data\n",
        "predicted = sales_scaler.inverse_transform(predicted_scaled)\n",
        "\n",
        "# Plot predictions\n",
        "plt.plot(sales_df[\"month\"], sales_df[\"sales\"], marker='o', linestyle='', alpha=0.3)\n",
        "plt.plot(sales_df[\"month\"], predicted, marker='', color='r')\n",
        "plt.title(\"monthly sales\")\n",
        "plt.xlabel(\"month\")\n",
        "plt.ylabel(\"sales\")\n",
        "plt.show()\n",
        "\n",
        "## 7. Measure error\n",
        "regression_error(sales_df[\"sales\"], predicted[\"sales\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There's our slight upward trend... and almost $\\$1000$ of average error.\n",
        "\n",
        "### Why we look at data\n",
        "\n",
        "Let's say that when we first plotted our data, instead of plotting dots with:\n",
        "\n",
        "```py\n",
        "plt.plot(sales_df[\"month\"], sales_df[\"sales\"], marker='o', linestyle='', alpha=0.3)\n",
        "```\n",
        "\n",
        "we left out some of the parameters and plotted this instead:\n",
        "\n",
        "```py\n",
        "plt.plot(sales_df[\"month\"], sales_df[\"sales\"], alpha=0.3)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(sales_df[\"month\"], sales_df[\"sales\"], alpha=0.3)\n",
        "plt.title(\"monthly sales\")\n",
        "plt.xlabel(\"month\")\n",
        "plt.ylabel(\"sales\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Same Data, Different Story\n",
        "\n",
        "We can now see that the ups and downs in the sales amounts aren't random, but probably correlate with the months of the year.\n",
        "\n",
        "Whenever we want to model data like this, we have to add a special type of feature to our data in order to capture the ups and downs of cyclic, periodic values.\n",
        "\n",
        "Just like we added new quadratic and cubic features to our linear regression equation, like this:\n",
        "\n",
        "$\\displaystyle sales = \\beta_0 \\cdot month + \\beta_1 \\cdot month^{2} + \\beta_2 \\cdot month^{3}$\n",
        "\n",
        "We can add a periodic feature by using periodic functions like `sin()` and `cos()`:\n",
        "\n",
        "$\\displaystyle sales = \\beta_0 \\cdot month + \\beta_1 \\cdot \\sin(month)$\n",
        "\n",
        "We just have to define a function and then create a new column in our `DataFrame` based on this function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from math import sin, pi\n",
        "\n",
        "# function with a period of 12 (months)\n",
        "def sinx(x):\n",
        "  return sin(2 * x * pi / 12.0)\n",
        "\n",
        "# create a new column called \"periodic_month\" by applying sinx() to all values of month\n",
        "sales_df[\"periodic_month\"] = sales_df[\"month\"].apply(sinx)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Re-Model\n",
        "\n",
        "Everything from now on is the same. We can even copy+paste the code from above:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: train a Linear Regression predictor for sales based on month\n",
        "\n",
        "# this can be exactly the same as the other cell above, as long as\n",
        "# the new \"periodic_month\" feature is included in the data for the model\n",
        "\n",
        "## 3. Normalize\n",
        "sales_scaler = StandardScaler()\n",
        "sales_scaled = sales_scaler.fit_transform(sales_df)\n",
        "\n",
        "## 4. Separate the outcome variable and the independent variables\n",
        "sales = sales_scaled[\"sales\"]\n",
        "months = sales_scaled[[\"month\", \"periodic_month\"]]\n",
        "# or\n",
        "months = sales_scaled.drop(columns=[\"sales\"])\n",
        "\n",
        "## 5. Create a LinearRegression object\n",
        "sales_model = LinearRegression()\n",
        "\n",
        "# Use the object to create a model that relates sales amounts to month\n",
        "sales_model.fit(months, sales)\n",
        "\n",
        "## 6. Run the model on the training data\n",
        "predicted_scaled = sales_model.predict(months)\n",
        "\n",
        "# Un-normalize the data\n",
        "predicted = sales_scaler.inverse_transform(predicted_scaled)\n",
        "\n",
        "# Plot predictions\n",
        "plt.plot(sales_df[\"month\"], sales_df[\"sales\"], marker='o', linestyle='', alpha=0.3)\n",
        "plt.plot(sales_df[\"month\"], predicted, marker='', color='r')\n",
        "plt.title(\"monthly sales\")\n",
        "plt.xlabel(\"month\")\n",
        "plt.ylabel(\"sales\")\n",
        "plt.show()\n",
        "\n",
        "## 7. Measure error\n",
        "regression_error(sales_df[\"sales\"], predicted[\"sales\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And, just like that, our model's error got better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## More Regression!\n",
        "\n",
        "Let's do one more regression exercise using a different dataset.\n",
        "\n",
        "This one is for wine quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 1. Load Dataset\n",
        "WINE_FILE = \"https://raw.githubusercontent.com/DM-GY-9103-2024F-H/9103-utils/main/datasets/json/wines.json\"\n",
        "\n",
        "# Read into DataFrame\n",
        "wines_data = object_from_json_url(WINE_FILE)\n",
        "wines_df = pd.DataFrame.from_records(wines_data)\n",
        "\n",
        "# Look at features: values, types, names, etc\n",
        "wines_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Repeat some of the steps above to analyze the data\n",
        "\n",
        "Specifically, steps $3$: normalize and look at the covariance matrix using `quality` as the independent variable of interest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: normalize and look at covariance matrix\n",
        "\n",
        "## 3. Normalize\n",
        "wines_scaler = StandardScaler()\n",
        "wines_scaled = wines_scaler.fit_transform(wines_df)\n",
        "\n",
        "# Since this is a new dataset, let's just peek at its covariance matrix\n",
        "display(wines_scaled.cov())\n",
        "\n",
        "display(wines_scaled.cov()[\"quality\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot\n",
        "\n",
        "Looks like `alcohol`, `acidity`, `density` and `chlorides` are the $4$ features that mostly contribute to the quality.\n",
        "\n",
        "Let's look at graphs of `quality` as a function of these $4$ features.\n",
        "\n",
        "This could be two $3D$ graphs of pairs of variables, but four $2D$ graphs is probably easier to read."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: plot quality as a function of alcohol, acidity, density and chlorides\n",
        "\n",
        "for f in [\"alcohol\", \"acidity\", \"density\", \"chlorides\"]:\n",
        "  plt.scatter(wines_scaled[f], wines_scaled[\"quality\"], marker='o', linestyle='', alpha=0.3)\n",
        "  plt.xlabel(f)\n",
        "  plt.ylabel(\"quality\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Regression\n",
        "\n",
        "Use the method above that we used in the diamond dataset to create a model that predicts wine `quality` as a function of **ALL** of its other features.\n",
        "\n",
        "Use all of our features to run regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Create a linear model and run regression\n",
        "\n",
        "## 4. Separate the outcome variable and the independent variables\n",
        "quality = wines_scaled[\"quality\"]\n",
        "feats = wines_scaled.drop(columns=[\"quality\"])\n",
        "\n",
        "## 5. Create a LinearRegression object\n",
        "quality_model = LinearRegression()\n",
        "\n",
        "# Create a model that relates quality of wines to all other features\n",
        "quality_model.fit(feats, quality)\n",
        "\n",
        "## 6. Run the model on the training data\n",
        "predicted_scaled = quality_model.predict(feats)\n",
        "\n",
        "# Un-normalize the data\n",
        "predicted = wines_scaler.inverse_transform(predicted_scaled)\n",
        "\n",
        "## 7. Measure error\n",
        "regression_error(wines_df[\"quality\"], predicted[\"quality\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot Results\n",
        "\n",
        "On average our predictions are within $0.77$ points of the real quality values.\n",
        "\n",
        "Save the original `quality` values in a variable called `quality_original` and the `predicted quality` in another variable, called`quality_predicted`, and run the cell below to look at some plots of our predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot quality vs alcohol and volatile acidity\n",
        "for feat in [\"alcohol\", \"acidity\", \"density\", \"chlorides\"]:\n",
        "  x = wines_df[feat]\n",
        "  quality_original = wines_df[\"quality\"]\n",
        "  quality_predicted = predicted[\"quality\"]\n",
        "\n",
        "  # Plot the original quality\n",
        "  plt.plot(x, quality_original, marker='o', linestyle='', alpha=0.3)\n",
        "  plt.plot(x, quality_predicted, color='r', marker='o', linestyle='', alpha=0.3)\n",
        "  plt.xlabel(feat)\n",
        "  plt.ylabel(\"quality\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "# 🤔\n",
        "\n",
        "Hmm.... these could be better.\n",
        "\n",
        "Our model wasn't able to capture the fact that the calculated `quality` value should be a discrete value and not a number with decimals.\n",
        "\n",
        "This is because our `quality` category is not continuous, and instead can only have particular discrete values.\n",
        "\n",
        "Instead of trying to calculate continuous values for `quality`, our model should really be trying to put the wines in the right `quality` category.\n",
        "\n",
        "Let's use a different type of model for this task.\n",
        "\n",
        "Instead of learning how to predict a continuous value from the independent variables, like this:\n",
        "\n",
        "<img src=\"./imgs/wages-exp-fit.png\" width=\"620px\"/>\n",
        "\n",
        "Our model should learn how to place data points into discrete groups, like this:\n",
        "\n",
        "<img src=\"./imgs/wages-exp-classes.png\" width=\"620px\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classification\n",
        "\n",
        "This is what's called a *classification* problem, or task (sometimes also called _Logistic Regression_).\n",
        "\n",
        "Instead of trying to model the behavior of a continuous value, like `price` or `temperature`, a classification model tries to predict the correct *label* for given input data.\n",
        "\n",
        "The steps for training a classification model are the same:\n",
        "\n",
        "1. Load dataset\n",
        "2. Encode label features as numbers\n",
        "3. Normalize the data\n",
        "4. Separate the outcome variable and the feature variables\n",
        "5. Create a model\n",
        "6. Run model on input data and test data, and \n",
        "7. Measure error\n",
        "\n",
        "Even though we are trying to predict labels for our data, we still have to encode all label/categorical features into numbers, whether they are input or output variables. This is because our models are always using some kind of math to compare values and make predictions, and they wouldn't really know how to do that with text or other non-numeric data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random ? Forest ?\n",
        "\n",
        "The particular classification model we will use is called a [RandomForestClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html).\n",
        "\n",
        "The model gets its name from another type of model called a [Decision Tree](https://scikit-learn.org/stable/modules/tree.html). During training, Decision Trees learn to model our data using simple decision rules, in a process that is conceptually similar to the game [Twenty Questions](https://en.wikipedia.org/wiki/Twenty_questions).\n",
        "\n",
        "It learns to separate our data into categories by *asking* a series of yes/no, if/else, questions using our features:\n",
        "\n",
        "<img src=\"./imgs/decision-trees.jpg\" width=\"720px\"/>\n",
        "\n",
        "A Random Forest Classifier is a model that combines a bunch of Tree models that were trained with randomly selected subsets of our features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Let's Model !\n",
        "\n",
        "Steps are the same as before:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 4. Separate the outcome variable and the independent variables\n",
        "\n",
        "# We're still using scaled feature variables\n",
        "features = wines_scaled.drop(columns=[\"quality\"])\n",
        "\n",
        "# But, now our quality variable will be unscaled,\n",
        "# so it keeps its original values as labels 0 - 9\n",
        "quality = wines_df[\"quality\"]\n",
        "\n",
        "## 5. Create a Classifier object\n",
        "quality_model = RandomForestClassifier()\n",
        "\n",
        "# Create a model that classifies quality of wines based on many features\n",
        "quality_model.fit(features, quality)\n",
        "\n",
        "## 6. Run the model on the training data\n",
        "predicted = quality_model.predict(features)\n",
        "\n",
        "## 7. Measure error\n",
        "regression_error(wines_df[\"quality\"], predicted[\"quality\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot Results\n",
        "\n",
        "Ohh, that's better. On average, the model misses the quality value by about $0.1$ points.\n",
        "\n",
        "Let's take a look at some plots to confirm that the model captured the discrete-ness of our label:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot quality vs alcohol and volatile acidity\n",
        "for feat in [\"alcohol\", \"acidity\", \"density\", \"chlorides\"]:\n",
        "  x = wines_df[feat]\n",
        "  quality_original = wines_df[\"quality\"]\n",
        "  quality_predicted = predicted[\"quality\"]\n",
        "\n",
        "  # Plot the original quality\n",
        "  plt.plot(x, quality_original, marker='o', linestyle='', alpha=0.3)\n",
        "  plt.plot(x, quality_predicted, color='r', marker='o', linestyle='', alpha=0.3)\n",
        "  plt.xlabel(feat)\n",
        "  plt.ylabel(\"quality\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 🍷🎉\n",
        "\n",
        "Much better !\n",
        "\n",
        "### Test Dataset\n",
        "\n",
        "One thing that we haven't been doing, but is an important step when training any kind of ML model, is to actually test our model on data that wasn't used in the training.\n",
        "\n",
        "When we calculate the regression/prediction error on the same dataset that we use to create the model, it tells us how well the model learned from the data. When we calculate this error on a test dataset, one that wasn't used for fitting the model, it tells us how well the model can generalize its predictions to data it hasn't seen.\n",
        "\n",
        "The regression/prediction error is an indication of how good our model is.\n",
        "\n",
        "Calculating it using a separate test dataset will tell us whether our model has actually learned patterns from the data or just memorized the training data.\n",
        "\n",
        "The steps for preparing the data and running it through the model are the same, except we don't have to `fit()` a scaler, nor a model. We'll just use the scaler and model that we trained with the `train` set to `transform()`/`predict()` the `test` data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 1. Load Dataset\n",
        "WINE_TEST_FILE = \"https://raw.githubusercontent.com/DM-GY-9103-2024F-H/9103-utils/main/datasets/json/wines-test.json\"\n",
        "\n",
        "# Read into DataFrame\n",
        "wines_test_data = object_from_json_url(WINE_TEST_FILE)\n",
        "wines_test_df = pd.DataFrame.from_records(wines_test_data)\n",
        "\n",
        "## 3. Normalize\n",
        "wines_test_scaled = wines_scaler.transform(wines_test_df)\n",
        "\n",
        "## 4. Separate the independent variables\n",
        "features_test = wines_test_scaled.drop(columns=[\"quality\"])\n",
        "\n",
        "## 6. Run the model on the test data\n",
        "predicted_test = quality_model.predict(features_test)\n",
        "\n",
        "## 7. Measure error\n",
        "regression_error(wines_test_df[\"quality\"], predicted_test[\"quality\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpretation\n",
        "\n",
        "This is not great, but it's not bad. On average our model is within $0.55$ quality points of the real values.\n",
        "\n",
        "It's better than the result we originally got from linear regression on the test data ($0.76$).\n",
        "\n",
        "Let's take a look at some plots:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot quality vs alcohol and volatile acidity\n",
        "for feat in [\"alcohol\", \"acidity\", \"density\", \"chlorides\"]:\n",
        "  x = wines_test_df[feat]\n",
        "  quality_original = wines_test_df[\"quality\"]\n",
        "  quality_predicted = predicted_test[\"quality\"]\n",
        "\n",
        "  # Plot the original quality\n",
        "  plt.plot(x, quality_original, marker='o', linestyle='', alpha=0.3)\n",
        "  plt.plot(x, quality_predicted, color='r', marker='o', linestyle='', alpha=0.3)\n",
        "  plt.xlabel(feat)\n",
        "  plt.ylabel(\"quality\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conclusion\n",
        "\n",
        "We have an *ok* model for predicting wine quality based on a few parameters.\n",
        "\n",
        "The graphs don't really point to any obvious deficiencies in our model. It might be the best we can do for now with the features and data points that we have..."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPxe2qYxIG7EblrvD1C4Pmv",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.17 ('hf-model')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "89e384cab7c47fb35ec95d2248b519cf922ee174880eed636c26cdfb6c4df768"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
